{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Map (CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as trn\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from scipy.misc import imresize as imresize\n",
    "import cv2\n",
    "\n",
    "from Networks import classification as nets\n",
    "from Networks.StyleNet import StyleAugmentation\n",
    "from Networks.libs.Loader import Dataset\n",
    "\n",
    "import config_classification as conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyDir(dir_path):\n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "#model_path = \"models/\"\n",
    "model_path = \"StyleTransfer_Weights/\"\n",
    "output_path = \"output/\"\n",
    "\n",
    "verifyDir(output_path)\n",
    "\n",
    "Styles = [ \"\", \"SA-\", \"Aug-\", \"Aug+SA-\"]\n",
    "Layers = [\"conv2\", \"conv4\", \"conv4\", \"Mixed_7c\", \"branch3\"]\n",
    "Nets = [\"WideResNet101.pth\", \"Xception96.pth\", \"Xception256.pth\", \"InceptionV3.pth\", \"InceptionV4.pth\"]\n",
    "Names = [\"WideResNet101\", \"Xception96\", \"Xception256\", \"InceptionV3\", \"InceptionV4\"]\n",
    "sizes = [(96,8),(96,16),(256,16),(299,16),(299,16)]\n",
    "\n",
    "models_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, dims, name, layer in zip(Nets, sizes, Names, Layers):\n",
    "  for style in Styles:\n",
    "    models_weights.append({\"arch\": style+model, \"layer\": layer, \"style\": style, \"model\": name, \"dim\": [dims[0], dims[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'arch': 'Aug+SA-WideResNet101.pth', 'layer': 'conv2', 'style': 'Aug+SA-', 'model': 'WideResNet101', 'dim': [96, 96]}]\n"
     ]
    }
   ],
   "source": [
    "models_weights = [models_weights[:4][3]]\n",
    "\n",
    "print(models_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageTransform(img_dim=[224, 224]):\n",
    "  width, height = img_dim[0], img_dim[1]\n",
    "  # load the image transformer\n",
    "  tf = trn.Compose([trn.ToPILImage(), trn.Resize((width, height)), trn.ToTensor(), trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "  return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightsFeatures(model, layer_conv, model_name):\n",
    "  features_blobs = []\n",
    "  \n",
    "  def hook_feature(module, input, output):\n",
    "    features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
    "  # hook the feature extractor\n",
    "  features_names = [layer_conv] # this is the last conv layer of the resnet\n",
    "  \n",
    "  #i = 0\n",
    "  #for k, v in model.state_dict().items():\n",
    "  #  print(\"{} Layer {}\".format(i, k))\n",
    "  #  i = i+1\n",
    "  \n",
    "  #print(model)\n",
    "\n",
    "  for name in features_names:\n",
    "    if model_name == \"Xception\":\n",
    "      model._modules[name].register_forward_hook(hook_feature)\n",
    "    elif model_name == \"InceptionV4\":\n",
    "      model.features[20]._modules[name][0].register_forward_hook(hook_feature)\n",
    "    elif model_name == \"InceptionV3\":\n",
    "      model._modules.get(name).register_forward_hook(hook_feature)\n",
    "    elif model_name == \"WideResNet101\":\n",
    "      model.layer3[3]._modules[name].register_forward_hook(hook_feature)\n",
    "      \n",
    "  # get the softmax weight\n",
    "  params = list(model.parameters())\n",
    "  weight_softmax = params[-2].data.numpy()\n",
    "  weight_softmax[weight_softmax<0] = 0\n",
    "\n",
    "  return features_blobs, weight_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateImageMap(images, names_model, best_pred, img_name):\n",
    "  final_frame = []\n",
    "  height, width = 120, 380\n",
    "  blank_column = 255.0*np.ones(shape=[420, width, 3])\n",
    "  blank_row = 255.0*np.ones(shape=[height, 2560 + width, 3])\n",
    "  names_style = [\"N/A\", \"SA\", \"Trad\", \"Trad+SA\"]\n",
    "  for i in range(0, len(models_weights), 4):\n",
    "    final_frame.append(cv2.hconcat((blank_column, images[i], images[i+1], images[i+2], images[i+3])))\n",
    "\n",
    "  print(final_frame[0].shape)\n",
    "\n",
    "  final_cam = cv2.vconcat((blank_row, final_frame[0], final_frame[1], final_frame[2], final_frame[3], final_frame[4]))\n",
    "  \n",
    "  textSize = 1.2\n",
    "  thickness = 2\n",
    "  \n",
    "  # columna nombres de los modelos\n",
    "  for i in range(0,5):\n",
    "    y = 210 +420*(i) + height\n",
    "    if names_model[i*4] == \"InceptionV3\":\n",
    "      names_model[i*4] = \"InceptionV3-299\"\n",
    "    if names_model[i*4] == \"InceptionV4\":\n",
    "      names_model[i*4] = \"InceptionV4-299\"\n",
    "    if names_model[i*4] == \"Xception96\":\n",
    "      names_model[i*4] = \"Xception-96\"\n",
    "    if names_model[i*4] == \"Xception256\":\n",
    "      names_model[i*4] = \"Xception-256\"\n",
    "    cv2.putText(final_cam, names_model[i*4], (30, y), cv2.FONT_HERSHEY_COMPLEX, textSize, (255, 0, 0), thickness, cv2.LINE_AA)\n",
    "  \n",
    "  # fila nombre de los estilos\n",
    "  for i in range(0,4):\n",
    "    x = 320+640*i+width\n",
    "    cv2.putText(final_cam, names_style[i], (x, 70), cv2.FONT_HERSHEY_COMPLEX, textSize, (255, 0, 0), thickness, cv2.LINE_AA)\n",
    "  \n",
    "  # filas predicciones por cada modelo y cada estilo\n",
    "  for i in range(0, len(models_weights), 4):\n",
    "    print(best_pred[i], best_pred[i+1], best_pred[i+2], best_pred[i+3])\n",
    "    y = 420*int(i/4) + height + 40\n",
    "    for j in range(0,4):\n",
    "      x = 640*j + width + 40\n",
    "      print((i, x, y))\n",
    "      cv2.putText(final_cam, str(best_pred[i+j]), (x, y), cv2.FONT_HERSHEY_COMPLEX, textSize, (222, 231, 223), thickness, cv2.LINE_AA)\n",
    "  \n",
    "  cv2.imwrite(output_path+img_name+\"cam.jpg\", final_cam)\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(obj, model_name):\n",
    "  file_path = model_path+model_name+\"/\"+obj[\"arch\"]\n",
    "  #file_path = model_path+obj[\"arch\"]\n",
    "  checkpoint = torch.load(file_path)\n",
    "  print(\"\\nAccuracy: \", checkpoint[\"best_prec\"])\n",
    "  model = nets.ChooseNet(model_name, pretrained=conf)\n",
    "  state_dict = { str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items() }\n",
    "  model.load_state_dict(state_dict)\n",
    "  model.eval()\n",
    "  return model, checkpoint[\"best_prec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "  # generate the class activation maps upsample to 256x256\n",
    "  size_upsample = (256, 256)\n",
    "  nc, h, w = feature_conv.shape\n",
    "  print(\"Last conv shape: \", feature_conv.shape)\n",
    "  output_cam = []\n",
    "  for idx in class_idx:\n",
    "    cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    cam_img = np.uint8(255 * cam_img)\n",
    "    output_cam.append(imresize(cam_img, size_upsample))\n",
    "  return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCAM(img, img_name):\n",
    "  verifyDir(output_path)\n",
    "  images_CAM = []\n",
    "  names_CAM = []\n",
    "  acc_CAM = []\n",
    "  for obj in models_weights:\n",
    "    architecture = obj[\"arch\"]\n",
    "    style = obj[\"style\"]\n",
    "    layer_conv = obj[\"layer\"]\n",
    "    model_name = obj[\"model\"]\n",
    "    if model_name == \"Xception96\" or model_name == \"Xception256\":\n",
    "      model_name = \"Xception\"\n",
    "    model, best_pred = load_model(obj, model_name)\n",
    "    \n",
    "    print(\"Model: \", style+model_name)\n",
    "    \n",
    "    tf = imageTransform(obj[\"dim\"])\n",
    "    input_img = Variable(tf(img).unsqueeze(0))\n",
    "\n",
    "    features_blobs, weight_softmax = getWeightsFeatures(model, layer_conv, model_name)\n",
    "    \n",
    "    # forward pass\n",
    "    \n",
    "    logit = model.forward(input_img)\n",
    "    h_x = F.softmax(logit, 1).data.squeeze()\n",
    "    #h_x = F.cross_entropy(logit, 1).data.squeeze()\n",
    "    \n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.numpy()\n",
    "    idx = idx.numpy()\n",
    "    \n",
    "    print('Class activation map is saved ... ')\n",
    "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "    # render the CAM and output\n",
    "    img_cv2 = img\n",
    "    height, width, _ = img_cv2.shape\n",
    "    img_cv2 = cv2.resize(img_cv2, (256, 256))\n",
    "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(256, 256)), cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.4 + img_cv2 * 0.5\n",
    "    \n",
    "    cam_result = cv2.resize(result, (640, 420))\n",
    "    \n",
    "    images_CAM.append(cam_result)\n",
    "    names_CAM.append(style+obj[\"model\"])\n",
    "    acc_CAM.append(probs[0])\n",
    "    cv2.imwrite(output_path+img_name+architecture+'_cam.jpg', result)\n",
    "    \n",
    "  print(\"prediction:\", acc_CAM[0])\n",
    "  #generateImageMap(images_CAM, names_CAM, acc_CAM, img_name)\n",
    "  \n",
    "  return acc_CAM[0], images_CAM[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = str(random.randint(1, 10))\n",
    "dir_path = \"2\"\n",
    "mypath = \"img/\" + dir_path + \"/\"\n",
    "\n",
    "#styles_type = [78020, 8050, 1020, 6050, 100, 76300]\n",
    "styles_type = np.arange(10000)\n",
    "#alphas = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "alphas = [0.5] #np.arange(0.4, 0.7, 0.1)\n",
    "\n",
    "#WASM_images = []\n",
    "\n",
    "content_dataset = Dataset(mypath, 256,256,test=True)\n",
    "content_loader = torch.utils.data.DataLoader(dataset=content_dataset, batch_size=16, shuffle=False, num_workers=16, drop_last=True)\n",
    "Stylenet = Stylization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, (img, name) in enumerate(content_loader):\n",
    "  print(\"Image nro:\", it, \"img name:\", name[0])\n",
    "  if it == 1:\n",
    "    break\n",
    "  img_name = dir_path+\"_\"+str(name[0])\n",
    "\n",
    "  print(\"No Styling ... \")\n",
    "  img_nostyled = np.uint8(img[0].permute((1,2,0)).numpy()*255)[:,:,::-1]\n",
    "\n",
    "  cv2.imwrite(output_path+img_name+\".jpg\", img_nostyled)\n",
    "  acc_no, cam_no = testCAM(img_nostyled, img_name+\"_\")\n",
    "  \n",
    "  wasm_it = 0.0 - np.float32(np.multiply(acc_no, cam_no))\n",
    "  wasm_al = 0.0 - np.float32(np.multiply(acc_no, cam_no))\n",
    "  wasm_no = 0.0 - np.float32(cam_no)\n",
    "  \n",
    "  print(\"Styling ... \")\n",
    "  for style_type in styles_type:\n",
    "    for alpha in alphas:\n",
    "\n",
    "      print(\"Styling image: \", name[0],\" style number:\", style_type, \"intensity (alpha):\", alpha)\n",
    "      styled = Stylenet(img.cuda(), style_type, alpha)\n",
    "      img_styled = np.uint8(styled[0].permute((1,2,0)).cpu().numpy()*255)[:,:,::-1]\n",
    "      \n",
    "      cv2.imwrite(output_path+img_name+\"_\"+str(style_type)+\"_\"+str(alpha)+'_style.jpg', img_styled)\n",
    "      acc, cam = testCAM(img_styled, img_name+\"_\"+str(style_type)+\"_\"+str(alpha)+\"_\")\n",
    "      \n",
    "      wasm_it = wasm_it + np.float32(np.multiply(acc, cam))\n",
    "      wasm_al = wasm_al + np.float32(np.multiply(acc*alpha, cam))\n",
    "      wasm_no = wasm_no + np.float32(cam)\n",
    "\n",
    "  wasm_it = np.uint8((wasm_it - wasm_it.min())/(wasm_it.max() - wasm_it.min())*255.)\n",
    "  wasm_al = np.uint8((wasm_al - wasm_al.min())/(wasm_al.max() - wasm_al.min())*255.)\n",
    "  wasm_no = np.uint8((wasm_no - wasm_no.min())/(wasm_no.max() - wasm_no.min())*255.)\n",
    "  #wasm_it = cv2.normalize(cam_no, wasm_it, 0, 255, cv2.NORM_MINMAX)\n",
    "  #wasm_al = cv2.normalize(cam_no, wasm_al, 0, 255, cv2.NORM_MINMAX)\n",
    "  #wasm_no = cv2.normalize(cam_no, wasm_no, 0, 255, cv2.NORM_MINMAX)\n",
    "  \n",
    "  cv2.imwrite(output_path+\"wasm_\"+img_name+\"_\"+str(alpha)+'_style.jpg', wasm_it)\n",
    "  cv2.imwrite(output_path+\"wasm-al_\"+img_name+\"_\"+str(alpha)+'_style.jpg', wasm_al)\n",
    "  cv2.imwrite(output_path+\"wasm-no_\"+img_name+\"_\"+str(alpha)+'_style.jpg', wasm_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
